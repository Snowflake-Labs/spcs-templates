{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4e65ae-85a5-4f5a-81c1-d1b558959329",
   "metadata": {},
   "source": [
    "## Tutorial of using async batch jobs to run distributed data processing\n",
    "The tutorial if E2E example that creates new table, runs emotion classifier to understand text emotions and upload data back to the output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff73b4d-badd-447a-aa23-320065eff89a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install snowflake-snowpark-python\n",
    "!pip install snowflake-ml-python\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dda87-fdaa-4153-8fcd-d913bb93c7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "326f7e4a-377c-4854-8a6b-e6378e63cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "from snowflake.snowpark import Session\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d67fc1e4-74b6-4b47-8beb-4f3180316e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "account='' # YOUR_ACCOUNT\n",
    "user='' # YOUR_USER\n",
    "password='' # YOUR_PASSWORD\n",
    "role='' #YOUR_ROLE\n",
    "\n",
    "database='' #YOUR_DB\n",
    "schema='' #YOUR_SCHEMA\n",
    "\n",
    "warehouse='' #YOUR WH\n",
    "image_registry='project_repo' #name of the image registry that will be created\n",
    "image_name='classifier:1' # name of the image\n",
    "\n",
    "external_access_integration='' # EAI that is used to retrieve the model. It should have access to ('huggingface.co:443', 'cdn-lfs.hf.co:443');\n",
    "num_replicas = 10 # service number of replicas\n",
    "job_name='classifier_v1' # job name\n",
    "input_table='CLASSIFIER_DATA_INPUT' #input table\n",
    "output_table='CLASSIFIER_DATA_OUTPUT' #table to write results\n",
    "\n",
    "compute_pool_name='CLASSIFIER_TEST02' \n",
    "compute_pool_instance_family='CPU_X64_M'\n",
    "compute_pool_instances=10\n",
    "\n",
    "\n",
    "connection_parameters = {\n",
    "    \"account\": account,\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"warehouse\": warehouse,\n",
    "    \"database\": database,\n",
    "    \"schema\": schema,\n",
    "    \"role\": role,\n",
    "    \"client_session_keep_alive\": True,\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b819c2f-ef16-4f95-81b6-aa383f4068cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create EAI(optionally). NOTE: You need to have accountadmin permission to run the following code!!!!.\n",
    "\n",
    "# https://huggingface.co\n",
    "\n",
    "sql= f\"USE {database}.{schema}\"\n",
    "print(f\"executing: {sql}\")\n",
    "print(session.sql(sql).collect())\n",
    "\n",
    "sql=\"use role accountadmin\"\n",
    "print(f\"executing: {sql}\")\n",
    "print(session.sql(sql).collect())\n",
    "\n",
    "\n",
    "network_rule_sql=\"\"\"\n",
    "CREATE OR REPLACE NETWORK RULE hf_rule\n",
    "  MODE = EGRESS\n",
    "  TYPE = HOST_PORT\n",
    "  VALUE_LIST = ('huggingface.co:443', 'cdn-lfs.hf.co:443');\n",
    "\"\"\"\n",
    "\n",
    "print(f\"executing: {network_rule_sql}\")\n",
    "print(session.sql(network_rule_sql).collect())\n",
    "\n",
    "eai_sql= f\"\"\"\n",
    "CREATE EXTERNAL ACCESS INTEGRATION {external_access_integration}\n",
    "  ALLOWED_NETWORK_RULES = (hf_rule)\n",
    "   ENABLED = true;\n",
    "\"\"\"\n",
    "\n",
    "print(f\"executing: {eai_sql}\")\n",
    "print(session.sql(eai_sql).collect())\n",
    "\n",
    "sql=f\"GRANT USAGE ON INTEGRATION {external_access_integration} TO ROLE {role};\"\n",
    "print(session.sql(sql).collect())\n",
    "\n",
    "sql=f\"use role {role}\"\n",
    "print(session.sql(sql).collect())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c33f036-1b31-4f3d-bfc8-d35005b85f29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: CLASSIFIER_DATA_INPUT, with # of rows: 100000\n",
      "Table: CLASSIFIER_DATA_INPUT, finished batch: 0x50000\n",
      "Table: CLASSIFIER_DATA_INPUT, finished batch: 1x50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_file = 'english_words.txt'\n",
    "\n",
    "def get_words(filename):\n",
    "    with open(words_file) as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "def get_random_phrase(words, max_len=50):\n",
    "    plen = random.randint(1, max_len)\n",
    "    return \" \".join([words[random.randint(0, len(words) - 1)] for _ in range(plen)])\n",
    "\n",
    "\n",
    "def setup_table(session, full_table_name, recreate: bool = True,\n",
    "                 num_rows: int = 100000, batch_size=50000):\n",
    "    words = get_words(words_file)\n",
    "    overwrite = recreate\n",
    "    num_batches = num_rows // batch_size + 1\n",
    "    print(f\"Creating table: {full_table_name}, with # of rows: {num_rows}\")\n",
    "    row_idx = 0\n",
    "    for batch_idx in range(0, num_batches):\n",
    "        rows = []\n",
    "        for _ in range(batch_idx * batch_size, min(num_rows, (batch_idx + 1) * batch_size)):\n",
    "            rows.append({\"ID\": row_idx, \"TEXT\": get_random_phrase(words, max_len=50)})\n",
    "            row_idx += 1\n",
    "            \n",
    "        df = pd.DataFrame(rows)\n",
    "        if len(df) > 0:\n",
    "            session.write_pandas(df, full_table_name, auto_create_table=True, overwrite=overwrite)\n",
    "            print(f\"Table: {full_table_name}, finished batch: {batch_idx}x{batch_size}\")\n",
    "        overwrite = False\n",
    "\n",
    "\n",
    "setup_table(session, input_table, recreate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0263d-eecf-4800-bb8e-dd4c10b06852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_compute_pool_sql = f\"\"\"\n",
    "create compute pool if not exists {compute_pool_name}\n",
    "  min_nodes={compute_pool_instances}\n",
    "  max_nodes={compute_pool_instances}\n",
    "  instance_family={compute_pool_instance_family};\n",
    "\"\"\"\n",
    "\n",
    "print(session.sql(create_compute_pool_sql).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08218a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8758d39-2323-4769-80f0-8236aa6e53fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(status='CLASSIFIER_V1 successfully dropped.')]\n",
      "[Row(status=\"Started Snowpark Container Services Job 'CLASSIFIER_V1'.\")]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from snowflake.ml.jobs import submit_directory\n",
    "\n",
    "sql_retrieval_command = f\"select * from {input_table}\"\n",
    "\n",
    "job = submit_directory(\n",
    "    os.path.dirname(__file__),\n",
    "    compute_pool_name,\n",
    "    entrypoint=\"main.py\",\n",
    "    args=[\n",
    "        f'--sql=\"{sql_retrieval_command}\"',\n",
    "        f'--output-table={output_table}',\n",
    "        '--batch-size=512',\n",
    "    ],\n",
    "    stage_name=\"payload_stage\",\n",
    "    external_access_integrations=[external_access_integration],\n",
    "    query_warehouse=warehouse,\n",
    "    num_instances=num_replicas,  # FIXME: Coming soon, not available yet\n",
    ")\n",
    "\n",
    "print(f\"Started job {job.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e34bb-1a58-4f4e-831d-11e6ebb53b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(session.sql(f'DESC SERVICE {job_name}').collect())\n",
    "\n",
    "res= session.sql(f'SHOW SERVICE CONTAINERS IN SERVICE {job_name}').collect()\n",
    "\n",
    "for row in res:\n",
    "    print(f\"{row['service_name']}/{row['instance_id']}/{row['container_name']} - status: {row['status']}, message: {row['message']}\")\n",
    "\n",
    "print(f\"Job status: {job.status}\")\n",
    "for i in num_replicas:  # TODO: We should add an API for retrieving this info from the MLJob object\n",
    "    print(f\"Job instance {i} status: {job.get_instance_status(i)}\")  # FIXME: Coming soon, not available yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844eeb4e-bf45-4736-b37e-e395f5eabd52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job.show_logs()  # or print(job.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953deee-a965-4673-adc5-f2594558f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "records = session.sql(f\"select * from {output_table}\").collect()\n",
    "for record in records[0:10]:\n",
    "    print(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948daab5-d9b2-4e26-9c33-6a4f63ebbccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "715eeb9b-6e4e-44ea-a964-7abb28f2d0f5",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

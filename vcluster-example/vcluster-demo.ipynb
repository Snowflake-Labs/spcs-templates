{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f60172-127d-4f02-bf2a-9e602fc9e120",
   "metadata": {},
   "source": [
    "## vCluster Tutorial \n",
    "\n",
    "This is a tutorial to launch Snowflake vCluster feature. The target audience of this tutorial is anyone who did not have prior experience with either Snowflake or SPCS(Snowpark Container Services).\n",
    "\n",
    "As part of this tutorial we will:\n",
    "* Setup Snowflake resources like Database, Schema, Stage, Image Repository\n",
    "* Install snowflake cluster client(spcsclusterctl)\n",
    "* Build and push docker images to the Snowflake Image Repository\n",
    "* Provision vCluster and GPU compute pools\n",
    "* Run jobs, observe the job statuses and run exec commands towards the cluster\n",
    "\n",
    "\n",
    "NOTE: \n",
    "\n",
    "Before beginning the tutorial, make sure that you have:\n",
    "\n",
    "* Docker installed. You can follow: https://docs.docker.com/engine/install/ to install it\n",
    "* Installed kubectl: https://kubernetes.io/docs/tasks/tools/ (this will be used to access vCluster)\n",
    "* Have running python\n",
    "* Have Snowflake account, username and password and role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "983f9b76-e609-4e53-8948-844877d87d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prereqs\n",
    "\n",
    "# docker, version: 27.4.0\n",
    "# kubectl \n",
    "# spcsclusterctl installed\n",
    "\n",
    "# Snowflake account locator\n",
    "# Snowflake \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a69ca-f908-4299-9f79-d3384c11d23e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install torchvision\n",
    "!pip install snowflake-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "29d17e59-77b3-418b-ad9b-01389379a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 27.4.0, build bde2b89\n"
     ]
    }
   ],
   "source": [
    "# check docker version\n",
    "! docker --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c13c41c3-cc3a-4a03-b567-f08a06ab567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Snowflake connection variables\n",
    "# DO NOT CHANGE\n",
    "#-----\n",
    "SNOWFLAKE_VCLUSTER_HOST=\"snowflake.prod3.us-west-2.aws.snowflakecomputing.com\"\n",
    "#-----\n",
    "\n",
    "SNOWFLAKE_ACCOUNT=\"YOUR_LOCATOR\"\n",
    "SNOWFLAKE_USER=\"YOUR_LOCATOR\"\n",
    "SNOWFLAKE_PASSWORD=\"YOUR_PASSWORD\"\n",
    "\n",
    "# Snowflake data related variables\n",
    "SNOWFLAKE_DATABASE=\"YOUR_DB\"\n",
    "SNOWFLAKE_SCHEMA=\"YOUR_SCHEMA\"\n",
    "SNOWFLAKE_ROLE=\"YOUR_ROLE\"\n",
    "\n",
    "IMAGE_REPO_NAME='test_repo' # image repo is used to store docker images\n",
    "\n",
    "SNOWFLAKE_DATA_STAGE=\"DATA_STAGE\"\n",
    "STAGE_PATH=f\"{SNOWFLAKE_DATA_STAGE}/test-data\"\n",
    "\n",
    "# Snowflake vCluster related variables\n",
    "CLUSTER_NAME=\"TEST10\"\n",
    "WORKER_INSTANCE_TYPE=\"GPU_NV_S\"\n",
    "\n",
    "# Trainer job resources \n",
    "TRAINER_NUM_GPUS=\"1\"\n",
    "TRAINER_NUM_CPUS=\"4\"\n",
    "TRAINER_MEM_GI=\"10Gi\"\n",
    "\n",
    "snowflake_path=\"~/.snowflake\"\n",
    "snowflake_abs_path=os.path.expanduser(\"~/.snowflake\")\n",
    "\n",
    "# -------- exporting to env variables for convenience\n",
    "\n",
    "os.environ['SNOWFLAKE_ABS_PATH']=snowflake_abs_path\n",
    "\n",
    "# export parameters as env variables for convenience\n",
    "os.environ['IMAGE_REPO_NAME']=IMAGE_REPO_NAME\n",
    "\n",
    "os.environ['SNOWFLAKE_DATA_STAGE']=SNOWFLAKE_DATA_STAGE\n",
    "os.environ['STAGE_PATH']=STAGE_PATH\n",
    "\n",
    "os.environ['SNOWFLAKE_VCLUSTER_HOST']=SNOWFLAKE_VCLUSTER_HOST\n",
    "os.environ['SNOWFLAKE_ACCOUNT']=SNOWFLAKE_ACCOUNT\n",
    "os.environ['SNOWFLAKE_USER']=SNOWFLAKE_USER\n",
    "os.environ['SNOWFLAKE_PASSWORD']=SNOWFLAKE_PASSWORD\n",
    "\n",
    "os.environ['SNOWFLAKE_DATABASE']=SNOWFLAKE_DATABASE\n",
    "os.environ['SNOWFLAKE_SCHEMA']=SNOWFLAKE_SCHEMA\n",
    "os.environ['SNOWFLAKE_ROLE']=SNOWFLAKE_ROLE\n",
    "\n",
    "os.environ['CLUSTER_NAME']=CLUSTER_NAME\n",
    "os.environ['WORKER_INSTANCE_TYPE']=WORKER_INSTANCE_TYPE\n",
    "\n",
    "os.environ['TRAINER_NUM_GPUS']=TRAINER_NUM_GPUS\n",
    "os.environ['TRAINER_NUM_CPUS']=TRAINER_NUM_CPUS\n",
    "os.environ['TRAINER_MEM_GI']=TRAINER_MEM_GI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dcfe47-0c74-4b0c-837a-5d1e83059b47",
   "metadata": {},
   "source": [
    "## Setting up snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ccb42-45eb-4b5e-a5e0-bedfaa912263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create database and schema\n",
    "\n",
    "import snowflake.connector\n",
    "\n",
    "connection_parameters = {\n",
    "    \"account\": os.environ['SNOWFLAKE_ACCOUNT'],\n",
    "    \"user\": os.environ['SNOWFLAKE_USER'],\n",
    "    \"password\": os.environ['SNOWFLAKE_PASSWORD'],\n",
    "    \"role\": os.environ['SNOWFLAKE_ROLE'],\n",
    "    \"client_session_keep_alive\": True\n",
    "}\n",
    "\n",
    "if 'SNOWFLAKE_HOST' in os.environ:\n",
    "    connection_parameters['host'] = os.environ['SNOWFLAKE_HOST']\n",
    "\n",
    "def upload_files_to_stage(local_dir:str, stage_path:str):\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            remote_path = local_file_path.replace(local_dir, \"\")\n",
    "            \n",
    "            put_command = f\"PUT 'file://{local_file_path}' @{stage_path} AUTO_COMPRESS=TRUE OVERWRITE=TRUE\"\n",
    "            print(f\"Uploading: {local_file_path} to @{stage_path}\")\n",
    "            cur.execute(put_command)\n",
    "\n",
    "\n",
    "with snowflake.connector.connect(**connection_parameters) as conn:\n",
    "    cur = conn.cursor()\n",
    "    print(cur.execute(f\"CREATE DATABASE IF NOT EXISTS {SNOWFLAKE_DATABASE}\").fetchall())\n",
    "    print(cur.execute(f\"USE DATABASE {SNOWFLAKE_DATABASE}\").fetchall())\n",
    "    print(cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {SNOWFLAKE_SCHEMA}\").fetchall())\n",
    "    print(cur.execute(f\"USE SCHEMA {SNOWFLAKE_SCHEMA}\").fetchall())\n",
    "    print(cur.execute(f\"CREATE IMAGE REPOSITORY IF NOT EXISTS {IMAGE_REPO_NAME}\").fetchall())\n",
    "    image_repos = cur.execute(f\"show image repositories like '{IMAGE_REPO_NAME}'\").fetchall()\n",
    "    \n",
    "    # url is the fourth parameter\n",
    "    image_url = image_repos[0][4]\n",
    "        \n",
    "    # store data and train images for convenience\n",
    "    DATA_IMAGE_REPO=f\"{image_url}/temp/download_data:08\"\n",
    "    TRAIN_IMAGE_REPO=f\"{image_url}/temp/train:08\"\n",
    "    \n",
    "    os.environ['DATA_IMAGE_REPO']=DATA_IMAGE_REPO\n",
    "    os.environ['TRAIN_IMAGE_REPO']=TRAIN_IMAGE_REPO\n",
    "\n",
    "    # creating stage for dummy data\n",
    "    print(cur.execute(f\"CREATE STAGE IF NOT EXISTS  {SNOWFLAKE_DATA_STAGE} \").fetchall())\n",
    "\n",
    "    # download data to the local machine\n",
    "    dest_local_dir = \"./data\"\n",
    "    os.makedirs(dest_local_dir, exist_ok=True)\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    torchvision.datasets.CIFAR10(root=dest_local_dir, train=True, download=True, transform=transform)\n",
    "    torchvision.datasets.CIFAR10(root=dest_local_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "    upload_files_to_stage(f\"{dest_local_dir}/cifar-10-batches-py\", f\"{SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}.{STAGE_PATH}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce5bd0-1173-4c04-856c-17dde058b3bf",
   "metadata": {},
   "source": [
    "## Setting up vCluster\n",
    "The commands below set up vCluster in customer account.\n",
    "We use `spcsclusterctl` program to manage Snowflake vClusters. The current release is available in https://github.com/Snowflake-Labs/spcs-templates/releases/tag/v0.0.1 location.\n",
    "\n",
    "In the following sections we will download and install `spcsclusterctl` on the well-known location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c09ab6-e04c-4142-9599-a1c7f371178d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform: ['arm64']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# retrieve the proper spcscluster binary link\n",
    "output=!uname -m\n",
    "print(f'platform: {output}')\n",
    "platform = output[0]\n",
    "\n",
    "def get_spcscluster_link():\n",
    "    amd_link = \"https://github.com/Snowflake-Labs/spcs-templates/releases/download/v0.0.1/spcsclusterctl.linux_amd64\"\n",
    "    arm_link = \"https://github.com/Snowflake-Labs/spcs-templates/releases/download/v0.0.1/spcsclusterctl.darwin_amd64\"\n",
    "\n",
    "    if platform=='arm64':\n",
    "        return arm_link, 'spcsclusterctl.darwin_amd64'\n",
    "    else:\n",
    "        return amd_link, 'spcsclusterctl.linux_amd64'\n",
    "\n",
    "\n",
    "spcscluster_link, filename = get_spcscluster_link()\n",
    "os.environ['SPCSCLUSTER_LINK']=spcscluster_link\n",
    "os.environ['SPCSCLUSTER_FILENAME']=filename\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a2c1ece-12c6-4e84-8385-9954cbea494e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#download and install spcscluster binary\n",
    "\n",
    "wget -q -P ~/.snowflake $SPCSCLUSTER_LINK\n",
    "mv ~/.snowflake/$SPCSCLUSTER_FILENAME ~/.snowflake/spcsclusterctl\n",
    "chmod +x ~/.snowflake/spcsclusterctl\n",
    "export PATH=$SNOWFLAKE_ABS_PATH:$PATH\n",
    "\n",
    "# allow binary execution on mac os\n",
    "if [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n",
    "    if xattr -l ~/.snowflake/spcsclusterctl | grep -q \"com.apple.quarantine\"; then\n",
    "        xattr -d com.apple.quarantine ~/.snowflake/spcsclusterctl\n",
    "    fi    \n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb58c8a-4d3f-42db-9254-dda98dc666fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exporting to local PATH for convenience\n",
    "os.environ[\"PATH\"] = f\"{snowflake_abs_path}:\" + os.environ[\"PATH\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588197df-5236-422e-addf-54a31af19151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aivanou/.snowflake/spcsclusterctl\n"
     ]
    }
   ],
   "source": [
    "!which spcsclusterctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a91448-39b0-40a4-8fcb-48a84d04af47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# create cluster with $CLUSTER_NAME name\n",
    "!spcsclusterctl create-cluster --cluster=$CLUSTER_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc6d32-4a36-468c-8e3d-5b11eb20ceb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# list existing clusters\n",
    "!spcsclusterctl list-clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "216e806d-36d3-446a-b081-f11bb30173ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/05/16 13:02:47 unable to create compute pool: 002002 (42710): SQL compilation error:\n",
      "Object 'GPU_NV_S' already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add compute pool(a set of nodes) to a vCluster\n",
    "!spcsclusterctl create-compute-pool \\\n",
    "    --cluster=$CLUSTER_NAME \\\n",
    "    --compute-pool-name=$CLUSTER_NAME_$WORKER_INSTANCE_TYPE \\\n",
    "    --num-instances=1 \\\n",
    "    --instance-type=$WORKER_INSTANCE_TYPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e564980-051b-41a0-879d-defe81a4faac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                STATUS   ROLES    AGE   VERSION\n",
      "node-10-16-36-230   Ready    <none>   52s   v1.30.11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Examine nodes with type $WORKER_INSTANCE_TYPE\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- get nodes -l snowflake.com/instance-type-name=$WORKER_INSTANCE_TYPE\n",
    "\n",
    "# Example of the output:\n",
    "# NAME                 STATUS   ROLES    AGE    VERSION\n",
    "# node-10-16-113-122   Ready    <none>   121m   v1.30.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeb1ad2d-e13a-4eba-b108-c0ff9bd2295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export node name that we will be using to run jobs on\n",
    "NODE_HOSTNAME=\"node-10-16-36-230\"\n",
    "os.environ['NODE_HOSTNAME']=NODE_HOSTNAME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0ce6c-d593-4f91-9dc9-e0755b21cbdb",
   "metadata": {},
   "source": [
    "## Buiding and running data containers\n",
    "\n",
    "The cells below build `download-data` container that uploads data to the node  from the stage that was used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d2977ed-6508-47c0-b104-e470a5bdbb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!docker login $DATA_IMAGE_REPO -u $SNOWFLAKE_USER -p $SNOWFLAKE_PASSWORD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7451e960-0afe-4b9f-a90d-a74b2d5882a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfengineering-xaccounttest2.registry.snowflakecomputing.com/aivanoudb/public/test_repo/temp/download_data:08\n"
     ]
    }
   ],
   "source": [
    "!echo $DATA_IMAGE_REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c896d-8ee1-451f-8c78-b0859d25a07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build docker image\n",
    "!docker build \\\n",
    " --platform linux/amd64 \\\n",
    " -t $DATA_IMAGE_REPO \\\n",
    " -f ./download_data/Dockerfile ./download_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9cfcf-37fc-45fc-9ac0-a7f01dc39161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# push image to the Snowflake Image Registry\n",
    "!docker push $DATA_IMAGE_REPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e28b6f5-c797-4bde-b5b1-5d5ccb0ed6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Populate pod with environment variables and save it in `./download_data/pod.yaml` file\n",
    "!envsubst < ./download_data/pod.template.yaml > ./download_data/pod.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "753016b8-1431-4ffe-8faf-8359718775f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): error when deleting \"./download_data/pod.yaml\": jobs.batch \"download-data\" not found\n",
      "2025/05/16 15:35:43 exit status 1\n",
      "job.batch/download-data created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# delete previous execution, if any\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- delete -f ./download_data/pod.yaml\n",
    "# start new execution\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- apply -f ./download_data/pod.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa382d3-9091-43e9-80ee-c2f553a541c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list current pods in vCluster\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- get pods -A\n",
    "\n",
    "# Example of output\n",
    "# NAMESPACE     NAME                       READY   STATUS      RESTARTS   AGE\n",
    "# default       download-data-67kgt        1/1     Running     0          4s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c1f65-6d2d-4639-a2a7-9e6296014499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get logs of the job\n",
    "!spcsclusterctl kubectl --cluster=TEST10 -- logs download-data-fm947\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecedfd5d-f57a-4d78-8dc9-2994d9bfc350",
   "metadata": {},
   "source": [
    "## Building and running dummy job\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51c2a4-1c1b-4a20-8596-3d6490c167b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build an image\n",
    "# Note: This is not actually a trainer but an example of how to run trainer-like containers that have access to GPUs and hostpaths\n",
    "!docker build \\\n",
    " --platform linux/amd64 \\\n",
    " -t $TRAIN_IMAGE_REPO \\\n",
    " -f ./trainer/Dockerfile ./trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9c76e-4596-4170-85fe-5ff0d493e849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Push image to the image repository\n",
    "!docker push $TRAIN_IMAGE_REPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b048518-ce62-4840-a92b-913be73b12bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): error when deleting \"./trainer/pod.yaml\": jobs.batch \"trainer\" not found\n",
      "2025/05/16 15:40:18 exit status 1\n",
      "job.batch/trainer created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Populate pod with environment variables and save it in `./download_data/pod.yaml` file\n",
    "!envsubst < ./trainer/pod.template.yaml > ./trainer/pod.yaml\n",
    "\n",
    "# delete existing job if exists\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- delete -f ./trainer/pod.yaml\n",
    "\n",
    "# create new job\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- apply -f ./trainer/pod.yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b8c3c-60fd-4fd3-a8fd-0e8a7f2c6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- get pods -A\n",
    "\n",
    "# Expected output\n",
    "# NAMESPACE     NAME                       READY   STATUS      RESTARTS   AGE\n",
    "# default       download-data-67kgt        0/1     Completed   0          40m\n",
    "# default       trainer-f2npd              1/1     Running     0          10s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64730fa3-053a-45dd-b199-0c33f6cb3e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job.batch \"trainer\" deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Delete trainer command\n",
    "# !spcsclusterctl kubectl --cluster=TEST10 -- delete -f ./trainer/pod.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63a10696-8d51-46be-add8-b8a691b9b250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer id: trainer-bjx5j\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get trainer based on kubectl labels\n",
    "OUTPUT=!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- get pod -l workload-type=trainer -o jsonpath=\"{.items[0].metadata.name}\"\n",
    "TRAINER_ID=OUTPUT[0]\n",
    "os.environ['TRAINER_ID'] = TRAINER_ID\n",
    "\n",
    "print(f\"trainer id: {TRAINER_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6435f2-8536-4c58-bc60-2a9438c5c354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run nvidia-smi command inside a trainer\n",
    "# NOTE: WAIT UNTIL CONTAINER IS IN `RUNNING` state!!!!\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- exec -it $TRAINER_ID -- nvidia-smi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57ed57-1581-4bce-827f-1f4f59dde483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- exec -it $TRAINER_ID -- ls /data/v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f7bdd-8c55-406a-b103-c3d362fc285d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get trainer logs\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- logs $TRAINER_ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d575110-f386-4f20-b0c7-9d8a0bba08dd",
   "metadata": {},
   "source": [
    "## Quick iteration between local and remote environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "7d4830e0-c722-4ae0-94da-fdbe7a7b5d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\n",
      " data\t\t download_data.py   trainer\t\t '~'\n",
      " download_data\t stanford_beacon    vcluster-demo.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Local directory to copy, change accordingly!\n",
    "local_dir = \"/Users/aivanou/code/spcs-templates/stanford_beacon/\"\n",
    "os.environ['LOCAL_DIR_TO_COPY']=local_dir\n",
    "\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- cp $LOCAL_DIR_TO_COPY $TRAINER_ID:/app/code\n",
    "\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- exec -it $TRAINER_ID ls /app/code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "4d7872cf-44e8-4d61-8c12-15abb2ee658c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\n",
      "E0305 15:50:33.777110   21327 websocket.go:296] Unknown stream id 1, discarding message\n",
      "Running dummy workload\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/app/main.py\", line 33, in <module>\n",
      "    time.sleep(2)\n",
      "KeyboardInterrupt\n",
      "command terminated with exit code 130\n",
      "2025/03/05 15:50:38 exit status 130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#run command remotely\n",
    "!spcsclusterctl kubectl --cluster=$CLUSTER_NAME -- exec -it $TRAINER_ID python /app/main.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc113d-20f3-4874-a765-2496fe46a63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dev-3.10)",
   "language": "python",
   "name": "dev-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
